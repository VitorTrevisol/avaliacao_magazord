{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5fb3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 22:38:42,639 - INFO - Conectado ao PostgreSQL com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "import logging\n",
    "import os\n",
    "\n",
    "MONGO_URI = os.getenv('MONGO_URI')\n",
    "POSTGRES_URI = os.getenv('POSTGRES_URI')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "# MongoDB\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[\"raw_data\"]\n",
    "\n",
    "# PostgreSQL\n",
    "DB_USER = \"user_analytics\"\n",
    "DB_PASS = \"password123\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"analytics_db\"\n",
    "\n",
    "engine = create_engine(POSTGRES_URI)\n",
    "\n",
    "# Teste de conexÃ£o\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        logging.info(\"Conectado ao PostgreSQL com sucesso!\")\n",
    "except Exception as e:\n",
    "    logging.info(\"Erro:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc82a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter_data_hibrida(coluna):\n",
    "    \"\"\"Trata Unix, ISO, SQL e Strings BR (DD/MM) sem erro de overflow.\"\"\"\n",
    "    num_vals = pd.to_numeric(coluna, errors='coerce')\n",
    "    mask_unix = (num_vals > 0) & (num_vals < 4102444800)\n",
    "    \n",
    "    d_num = pd.to_datetime(num_vals[mask_unix], unit='s', utc=True)\n",
    "    d_str = pd.to_datetime(coluna, errors='coerce', dayfirst=True, utc=True)\n",
    "    \n",
    "    return d_num.reindex(coluna.index).fillna(d_str).dt.tz_localize(None)\n",
    "\n",
    "def limpar_tipos_complexos(df):\n",
    "    \"\"\"Converte listas/dicionÃ¡rios em string para compatibilidade SQL.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "            df[col] = df[col].astype(str)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d4fb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 22:38:42,713 - INFO - Banco de dados resetado.\n"
     ]
    }
   ],
   "source": [
    "with engine.begin() as conn:\n",
    "    for tbl in inspect(engine).get_table_names():\n",
    "        conn.execute(text(f'DROP TABLE IF EXISTS \"{tbl}\" CASCADE;'))\n",
    "    logging.info(\"Banco de dados resetado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67409412",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_raw = pd.json_normalize(list(db[\"users\"].find())).drop(columns=['_id'], errors='ignore')\n",
    "u_raw.columns = [c.replace('.', '_') for c in u_raw.columns]\n",
    "\n",
    "# DIM_LOCATIONS\n",
    "geo_cols = ['address_city', 'address_state', 'address_stateCode', 'address_country']\n",
    "df_locs = u_raw[geo_cols].drop_duplicates().reset_index(drop=True)\n",
    "df_locs['location_id'] = df_locs.index\n",
    "df_locs['state_fmt'] = df_locs['address_state'] + \" (\" + df_locs['address_stateCode'] + \")\"\n",
    "u_raw = u_raw.merge(df_locs, on=geo_cols)\n",
    "dim_locations = df_locs[['location_id', 'address_city', 'state_fmt', 'address_country']].rename(\n",
    "    columns={'address_city': 'city', 'state_fmt': 'state', 'address_country': 'country'}\n",
    ")\n",
    "\n",
    "# DIM_COMPANY\n",
    "comp_cols = [c for c in u_raw.columns if c.startswith('company_')]\n",
    "dim_company = u_raw[comp_cols].drop_duplicates().reset_index(drop=True)\n",
    "dim_company['company_id'] = dim_company.index\n",
    "u_raw = u_raw.merge(dim_company, on=comp_cols)\n",
    "\n",
    "# DIM_FINANCE\n",
    "fin_cols = [c for c in u_raw.columns if c.startswith('bank_') or c.startswith('crypto_')]\n",
    "dim_finance = u_raw[['id'] + fin_cols].copy().rename(columns={'id': 'user_id'})\n",
    "\n",
    "# DIM_USERS\n",
    "keep_u = ['id', 'firstName', 'lastName', 'maidenName', 'age', 'gender', 'email', 'phone', \n",
    "          'username', 'password', 'birthDate', 'role', 'cpf', 'cnpj', 'address_address', \n",
    "          'address_postalCode', 'address_coordinates_lat', 'address_coordinates_lng',\n",
    "          'location_id', 'company_id']\n",
    "dim_users = u_raw[keep_u]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57458d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_raw = pd.json_normalize(list(db[\"products\"].find())).drop(columns=['_id'], errors='ignore')\n",
    "\n",
    "p_raw['brand_raw'] = p_raw['brand'].fillna('').astype(str).str.strip()\n",
    "u_brands = sorted([b for b in p_raw['brand_raw'].unique() if b != ''])\n",
    "dim_brands = pd.DataFrame({\n",
    "    'brand_id': range(len(['NÃ£o Informado'] + u_brands)), \n",
    "    'brand_name': ['NÃ£o Informado'] + u_brands\n",
    "})\n",
    "b_map = dict(zip(dim_brands['brand_name'], dim_brands['brand_id']))\n",
    "p_raw['brand_id'] = p_raw['brand_raw'].apply(lambda x: b_map.get(x, 0))\n",
    "\n",
    "dim_products = p_raw.drop(columns=['brand', 'brand_raw'])\n",
    "dim_products.columns = [c.replace('.', '_') for c in dim_products.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fe1068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2162/1242857121.py:7: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  d_str = pd.to_datetime(coluna, errors='coerce', dayfirst=True, utc=True)\n"
     ]
    }
   ],
   "source": [
    "carts_raw = list(db[\"carts\"].find())\n",
    "df_c_base = pd.DataFrame(carts_raw)\n",
    "\n",
    "# fact_sales\n",
    "fact_sales = df_c_base.drop(columns=['products', '_id'], errors='ignore')\n",
    "fact_sales['transaction_date'] = converter_data_hibrida(fact_sales['transaction_date'])\n",
    "\n",
    "# fact_sales_items\n",
    "df_exp = df_c_base.explode('products')\n",
    "items_norm = pd.json_normalize(df_exp['products'])\n",
    "fact_sales_items = pd.concat([\n",
    "    df_exp[['id', 'userId']].rename(columns={'id': 'cart_id'}).reset_index(drop=True),\n",
    "    items_norm.rename(columns={'id': 'product_id'}).reset_index(drop=True)\n",
    "], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca6d200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 22:38:42,951 - INFO - dim_users carregada.\n",
      "2026-02-16 22:38:42,964 - INFO - dim_locations carregada.\n",
      "2026-02-16 22:38:42,978 - INFO - dim_company carregada.\n",
      "2026-02-16 22:38:42,998 - INFO - dim_finance carregada.\n",
      "2026-02-16 22:38:43,054 - INFO - dim_products carregada.\n",
      "2026-02-16 22:38:43,062 - INFO - dim_brands carregada.\n",
      "2026-02-16 22:38:43,091 - INFO - fact_sales carregada.\n",
      "2026-02-16 22:38:43,213 - INFO - fact_sales_items carregada.\n"
     ]
    }
   ],
   "source": [
    "tabelas = {\n",
    "    'dim_users': dim_users, 'dim_locations': dim_locations, \n",
    "    'dim_company': dim_company, 'dim_finance': dim_finance,\n",
    "    'dim_products': dim_products, 'dim_brands': dim_brands,\n",
    "    'fact_sales': fact_sales, 'fact_sales_items': fact_sales_items\n",
    "}\n",
    "\n",
    "for nome, df in tabelas.items():\n",
    "    limpar_tipos_complexos(df).to_sql(nome, engine, if_exists='replace', index=False)\n",
    "    logging.info(f\"{nome} carregada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbd9df86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 22:38:41,944 - INFO - ğŸ”— Integridade referencial estabelecida com sucesso!\n"
     ]
    }
   ],
   "source": [
    "with engine.begin() as conn:\n",
    "    # PKs\n",
    "    conn.execute(text('ALTER TABLE dim_users ADD PRIMARY KEY (id);'))\n",
    "    conn.execute(text('ALTER TABLE dim_products ADD PRIMARY KEY (id);'))\n",
    "    conn.execute(text('ALTER TABLE dim_locations ADD PRIMARY KEY (location_id);'))\n",
    "    conn.execute(text('ALTER TABLE dim_brands ADD PRIMARY KEY (brand_id);'))\n",
    "    conn.execute(text('ALTER TABLE dim_company ADD PRIMARY KEY (company_id);'))\n",
    "    conn.execute(text('ALTER TABLE fact_sales ADD PRIMARY KEY (id);'))\n",
    "\n",
    "    # FKs\n",
    "    conn.execute(text('ALTER TABLE dim_finance ADD CONSTRAINT fk_finance_user FOREIGN KEY (user_id) REFERENCES dim_users (id);'))\n",
    "    conn.execute(text('ALTER TABLE fact_sales_items ADD CONSTRAINT fk_items_user FOREIGN KEY (\"userId\") REFERENCES dim_users (id);'))\n",
    "    conn.execute(text('ALTER TABLE dim_users ADD CONSTRAINT fk_user_location FOREIGN KEY (location_id) REFERENCES dim_locations (location_id);'))\n",
    "    conn.execute(text('ALTER TABLE dim_users ADD CONSTRAINT fk_user_company FOREIGN KEY (company_id) REFERENCES dim_company (company_id);'))\n",
    "    conn.execute(text('ALTER TABLE dim_products ADD CONSTRAINT fk_product_brand FOREIGN KEY (brand_id) REFERENCES dim_brands (brand_id);'))\n",
    "    conn.execute(text('ALTER TABLE fact_sales_items ADD CONSTRAINT fk_items_sale FOREIGN KEY (cart_id) REFERENCES fact_sales (id);'))\n",
    "    conn.execute(text('ALTER TABLE fact_sales_items ADD CONSTRAINT fk_items_product FOREIGN KEY (product_id) REFERENCES dim_products (id);'))\n",
    "\n",
    "logging.info(\"ğŸ”— Integridade referencial estabelecida com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de321d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd00116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47bf037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a94c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d0838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c56bdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8123c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01624e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e870db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e804285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
