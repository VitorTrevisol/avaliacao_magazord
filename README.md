<div style="display: inline_block">
  <img align="center" height="30" width="40" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/python/python-original.svg" />
  <img align="center" height="30" width="40" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/pandas/pandas-original.svg" />
  <img align="center" height="30" width="40" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/postgresql/postgresql-original.svg" />
  <img align="center" height="30" width="40" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/mongodb/mongodb-original.svg" />
  <img align="center" height="30" width="40" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/docker/docker-original.svg" />
  <img align="center" height="30" width="40" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/git/git-original.svg" />
  <img align="center" height="30" width="40" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/linux/linux-original.svg" />
</div>

<br/>

---

## ğŸ“ Estrutura do Projeto

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database.py    # DefiniÃ§Ã£o de DDL e Schema (SQLAlchemy)
â”‚   â”œâ”€â”€ extract.py     # Leitura de coleÃ§Ãµes MongoDB
â”‚   â”œâ”€â”€ transform.py   # Regras de negÃ³cio e transformaÃ§Ã£o Star Schema
â”‚   â”œâ”€â”€ load.py        # LÃ³gica de Upsert e carga SQL
â”‚   â”œâ”€â”€ config.py      # ConfiguraÃ§Ãµes de tabelas e mapeamentos
â”‚   â””â”€â”€ utils.py       # FunÃ§Ãµes auxiliares e conversores
â”œâ”€â”€ img/               # Ativos visuais e grÃ¡ficos das anÃ¡lises
â”œâ”€â”€ main.py            # Orquestrador central do ETL
â”œâ”€â”€ queries.sql        # Consultas analÃ­ticas otimizadas
â”œâ”€â”€ docker-compose.yml # Infraestrutura como cÃ³digo
â””â”€â”€ requirements.txt   # DependÃªncias do projeto
```

---

## ğŸ—ï¸ Arquitetura e Tecnologias

O projeto foi estruturado para garantir escalabilidade e facilidade de manutenÃ§Ã£o:
* **Linguagem:** Python 3.10
* **Processamento de Dados:** Pandas (utilizado para transformaÃ§Ã£o e normalizaÃ§Ã£o)
* **ComunicaÃ§Ã£o DB:** SQLAlchemy & Psycopg2
* **ContainerizaÃ§Ã£o:** Docker & Docker Compose

---

## ğŸ“Š Modelagem de Dados (Star Schema)

* **Tabelas de Fato:**
    * `fact_sales`: ConsolidaÃ§Ã£o de cabeÃ§alhos de pedidos, totais e datas.
    * `fact_sales_items`: Granularidade ao nÃ­vel de item/SKU para anÃ¡lises de mix de produtos.
* **Tabelas de DimensÃ£o:**
    * `dim_users`: Atributos demogrÃ¡ficos e geogrÃ¡ficos dos clientes.
    * `dim_products`: InformaÃ§Ãµes detalhadas sobre o catÃ¡logo de produtos.
    * `dim_date`: DimensÃ£o de tempo gerada para facilitar filtros temporais (ano, mÃªs, dia da semana).

---

## ğŸ› ï¸ DecisÃµes de Engenharia e Boas PrÃ¡ticas

### 1. IdempotÃªncia (Upsert Logic)
O pipeline utiliza uma estratÃ©gia de **Upsert** baseada em tabelas temporÃ¡rias. Antes da carga final, os dados sÃ£o inseridos em uma `temp_table` e movidos para a tabela definitiva utilizando `ON CONFLICT (pk) DO UPDATE`. Isso garante que o estado do banco permaneÃ§a consistente sem duplicatas.

### 2. Tratamento de Dados e ResiliÃªncia
* **Datas HÃ­bridas:** ImplementaÃ§Ã£o da funÃ§Ã£o `converter_data_hibrida` que lida automaticamente com formatos variados (Unix Timestamp e strings ISO).
* **Schema Enforcement:** Uso de mapeamento (`TABLE_SCHEMAS`) para garantir que o DataFrame possua exatamente as colunas e tipos esperados pelo PostgreSQL.
* **Limpeza de Tipos Complexos:** ConversÃ£o automÃ¡tica de dicionÃ¡rios e listas aninhadas do JSON original para SQL.

### 3. Observabilidade
O pipeline utiliza o mÃ³dulo `logging` do Python:
* **INFO:** Registra inÃ­cio/fim de etapas e volumetria.
* **WARNING:** Alerta sobre inconsistÃªncias menores tratadas.
* **ERROR:** Reporta falhas crÃ­ticas de conexÃ£o ou esquema.

---

## ğŸš€ Como Executar

**PrÃ©-requisitos:** Docker e Docker Compose instalados.

1. **Clone o repositÃ³rio:**
   ```bash
   git clone [https://github.com/seu-usuario/data_engineer_test.git](https://github.com/seu-usuario/data_engineer_test.git)
   cd data_engineer_test
   ```

2. **Inicie o ambiente:**
   ```bash
   docker compose up -d --build
   ```

3. **Acompanhe o processamento:**
   ```bash
   docker logs -f etl_job
   ```

---

## ğŸ” AnÃ¡lises e Insights

### 1. Performance por Categoria e Estado
DistribuiÃ§Ã£o da receita entre as categorias de produtos e visÃ£o consolidada por regiÃ£o.

| Faturamento por Categoria | Faturamento por Estado |
|---|---|
| ![Faturamento por Receita](img/faturamento_por_receita.png) | ![Faturamento por Estado](img/faturamento_por_estado.png) |

### 2. EvoluÃ§Ã£o Mensal
Acompanhamento temporal para identificaÃ§Ã£o de sazonalidades.
![Faturamento Mensal](img/faturamento_mensal.png)

### 3. Curva de Pareto (Regra 80/20)
IdentificaÃ§Ã£o do grupo de produtos que representa a maior parte do faturamento.
![Pareto Chart](img/pareto_chart.png)

---

## ğŸ—„ï¸ Exemplo de Consulta AnalÃ­tica (Pareto)

```sql
WITH product_sales AS (
    SELECT 
        p.title,
        SUM(i.discountedtotal) as receita_total
    FROM fact_sales_items i
    JOIN dim_products p ON i.product_id = p.product_id
    GROUP BY p.title
),
pareto_calc AS (
    SELECT 
        title,
        receita_total,
        SUM(receita_total) OVER (ORDER BY receita_total DESC) as receita_acumulada,
        SUM(receita_total) OVER () as receita_global
    FROM product_sales
)
SELECT 
    title,
    receita_total,
    ROUND((receita_acumulada / receita_global) * 100, 2) as porcentagem_acumulada
FROM pareto_calc
ORDER BY receita_total DESC
LIMIT 20;
```
