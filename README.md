# Magazord Data Engineer Challenge - ETL Pipeline

Este repositÃ³rio contÃ©m a soluÃ§Ã£o desenvolvida para o desafio tÃ©cnico de Engenharia de Dados da **Magazord**. O objetivo principal Ã© a construÃ§Ã£o de um pipeline de dados robusto que migra dados de uma origem NoSQL (**MongoDB**) para um Data Warehouse relacional (**PostgreSQL**) modelado para fins analÃ­ticos.

## ğŸ“ Estrutura de Pastas

```text
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database.py    # DefiniÃ§Ã£o de DDL e Schema
â”‚   â”œâ”€â”€ extract.py     # Leitura de coleÃ§Ãµes MongoDB
â”‚   â”œâ”€â”€ transform.py   # Regras de negÃ³cio e Star Schema
â”‚   â”œâ”€â”€ load.py        # LÃ³gica de Upsert e carga SQL
â”‚   â”œâ”€â”€ config.py      # ConfiguraÃ§Ãµes de tabelas e mapeamentos
â”‚   â””â”€â”€ utils.py       # FunÃ§Ãµes auxiliares e conversores
â”œâ”€â”€ main.py            # Orquestrador central do ETL
â”œâ”€â”€ docker-compose.yml # Infraestrutura como cÃ³digo
â””â”€â”€ requirements.txt   # DependÃªncias do projeto

## ğŸ—ï¸ Arquitetura e Tecnologias

O projeto foi estruturado para garantir escalabilidade e facilidade de manutenÃ§Ã£o:

* **Linguagem:** Python 3.10
* **Processamento de Dados:** Pandas (utilizado para transformaÃ§Ã£o e normalizaÃ§Ã£o)
* **Banco de Dados de Origem:** MongoDB 8.0 (Raw Data)
* **Banco de Dados de Destino:** PostgreSQL 17 (Data Warehouse)
* **OrquestraÃ§Ã£o:** Docker & Docker Compose
* **ComunicaÃ§Ã£o DB:** SQLAlchemy & Psycopg2

## ğŸ“Š Modelagem de Dados (Star Schema)

Para suportar consultas analÃ­ticas de alto desempenho, os dados foram transformados em um modelo **Estrela (Star Schema)**:

* **Fatos:**
    * `fact_sales`: ConsolidaÃ§Ã£o de cabeÃ§alhos de pedidos, totais e datas.
    * `fact_sales_items`: Granularidade ao nÃ­vel de item/SKU para anÃ¡lises de mix de produtos.
* **DimensÃµes:**
    * `dim_users`: Atributos demogrÃ¡ficos e geogrÃ¡ficos dos clientes.
    * `dim_products`: InformaÃ§Ãµes detalhadas sobre o catÃ¡logo de produtos.
    * `dim_date`: DimensÃ£o de tempo gerada para facilitar filtros temporais (ano, mÃªs, dia da semana, trimestres).

---

## ğŸ› ï¸ DecisÃµes de Engenharia e Boas PrÃ¡ticas

### 1. IdempotÃªncia (Upsert Logic)
O pipeline utiliza uma estratÃ©gia de **Upsert** baseada em tabelas temporÃ¡rias. Antes da carga final, os dados sÃ£o inseridos em uma `temp_table` e movidos para a tabela definitiva utilizando `ON CONFLICT (pk) DO UPDATE`. Isso garante que, se o script rodar mÃºltiplas vezes, o estado do banco permaneÃ§a consistente sem duplicatas.

### 2. Tratamento de Dados e ResiliÃªncia
* **Datas HÃ­bridas:** ImplementaÃ§Ã£o da funÃ§Ã£o `converter_data_hibrida` que lida automaticamente com formatos variados (Unix Timestamp e strings ISO) encontrados no MongoDB.
* **Schema Enforcement:** Uso de um dicionÃ¡rio de mapeamento (`TABLE_SCHEMAS`) para garantir que o DataFrame final possua exatamente as colunas e tipos esperados pelo PostgreSQL.
* **Limpeza de Tipos Complexos:** ConversÃ£o automÃ¡tica de dicionÃ¡rios e listas aninhadas do JSON original em strings/objetos compatÃ­veis com SQL.

### 3. Observabilidade
O pipeline utiliza o mÃ³dulo `logging` do Python para fornecer visibilidade sobre cada etapa do processo:
* `INFO`: Registra o inÃ­cio/fim de cada etapa e volumetria processada.
* `WARNING`: Alerta sobre datas invÃ¡lidas ou inconsistÃªncias menores que foram tratadas.
* `ERROR`: Reporta falhas crÃ­ticas de conexÃ£o ou esquema.

---

## ğŸš€ Como Executar

**PrÃ©-requisitos:** Ter o [Docker](https://www.docker.com/) e o Docker Compose instalados.

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/seu-usuario/data_engineer_test.git](https://github.com/seu-usuario/data_engineer_test.git)
    cd data_engineer_test
    ```

2.  **Inicie o ambiente:**
    ```bash
    docker-compose up -d --build
    ```
    *Este comando subirÃ¡ o MongoDB, o PostgreSQL e dispararÃ¡ automaticamente o container `etl_job` que processa os dados.*

3.  **Acompanhe o processamento:**
    ```bash
    docker logs -f etl_job
    ```

---

## ğŸ” AnÃ¡lises DisponÃ­veis

Dentro do repositÃ³rio, vocÃª encontrarÃ¡:

* **`queries.sql`**: Consultas otimizadas para o DW, incluindo:
    * Faturamento mensal e acumulado.
    * AnÃ¡lise de Pareto de produtos.
    * Ticket mÃ©dio por estado brasileiro.
* **`analises.ipynb`**: Notebook com visualizaÃ§Ãµes estatÃ­sticas (Boxplots) sobre o comportamento de compra por faixa etÃ¡ria, facilitando a identificaÃ§Ã£o de outliers e padrÃµes de consumo.

---



## ğŸ“ˆ Insights e VisualizaÃ§Ãµes

As anÃ¡lises foram geradas processando os dados do Data Warehouse e exportadas via Jupyter Notebook.

### DistribuiÃ§Ã£o de Vendas por Faixa EtÃ¡ria
O grÃ¡fico abaixo permite identificar o comportamento de compra e o ticket mÃ©dio de cada grupo demogrÃ¡fico, auxiliando na segmentaÃ§Ã£o de campanhas de marketing.

![Boxplot Idade](img/boxplot_idade.png)

### Curva de Pareto (Produtos)
AnÃ¡lise de concentraÃ§Ã£o de receita (Regra 80/20), identificando quais produtos representam a maior parte do faturamento da plataforma.

![Pareto Chart](img/pareto_chart.png)